# Smart_Routing_Agent/models/fuser.py
"""
The fuser algorithm’s goal is to take multiple candidate responses generated by different models and then “fuse” them into one final, coherent answer. 

Input Construction:
The algorithm starts by building a single input text that combines the original instruction (or prompt) with all of the candidate answers. It does this by concatenating:

    A header like "Instruction: {prompt}"
    A list of candidate answers (each preceded by a dash, e.g. "- {candidate}")
    A trailing marker "Fused Answer:" which tells the model that it now needs to generate a final response.
This structure helps the generative model understand that it should integrate the strengths of each candidate.

Provider Check and Generation:
The algorithm checks which provider the fuser model belongs to:
    API-based Providers (e.g., OpenAI, Anthropic):
        If the fuser model is provided by OpenAI or Anthropic, it sends the concatenated text as a message to the corresponding API using a chat-completion endpoint. 
        The API then generates a fused answer based on its internal language understanding.
    Local Models:
        If the fuser model is a local model (for example, a Hugging Face model), the algorithm tokenizes the concatenated input using an appropriate tokenizer. It then feeds these tokens to the model’s generate function, 
        which produces a sequence of tokens representing the fused answer. 
        Finally, the tokens are decoded back into human‑readable text.

Return Final Answer:
    After the generation step (whether via API or locally), the fused answer is extracted (or decoded) from the model’s output and returned as the final, combined answer.

In essence, the fuser takes the individual strengths of each candidate—each candidate might highlight different aspects of the answer—and uses a powerful language model to synthesize them into a single, 
improved response. This approach leverages the model’s ability to understand context, compare different responses, and generate a new output that ideally captures the best parts of each candidate.

This method is relatively straightforward but very powerful: by designing a clear input format and using a robust generative model (or API), 
the fuser can create an answer that is more comprehensive and refined than any single candidate alone.

"""
import os
import torch

import os
import torch
from serverRouter.core.models import get_model_by_id, ModelProvider
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

class Fuser:
    def __init__(self, config):
        self.device = config.device
        self.model_info = get_model_by_id(config.fuser_model_name)
        if self.model_info is None:
            raise ValueError(f"Fuser model {config.fuser_model_name} not found in registry.")
        if self.model_info.provider in [ModelProvider.OPENAI, ModelProvider.ANTHROPIC]:
            self.tokenizer = None
            self.model = None
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(config.fuser_model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(config.fuser_model_name)
            self.model.to(self.device)
    
    def fuse(self, prompt: str, candidates: list, max_new_tokens: int = 50) -> str:
        input_text = f"Instruction: {prompt}\nCandidates:\n"
        for cand in candidates:
            input_text += f"- {cand}\n"
        input_text += "\nFused Answer:"
        if self.model_info.provider in [ModelProvider.OPENAI, ModelProvider.ANTHROPIC]:
            from openai import OpenAI
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            response = client.chat.completions.create(
                model=self.model_info.name,
                messages=[{"role": "user", "content": input_text}],
                max_tokens=max_new_tokens,
            )
            return response.choices[0].message.content.strip()
        else:
            inputs = self.tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            with torch.no_grad():
                output_ids = self.model.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=4)
            fused_answer = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
            return fused_answer
